{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77be07b6-b85e-488d-9938-b9fa1c4c39b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ca56024-5e09-4fbe-8f1e-9bd2fafb8892",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(\".\", download=True, transform=T.ToTensor())\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=128,\n",
    "                                         shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fb463f7-092a-4706-857c-d72eb11bffa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generator Net\n",
    "class G(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(100, 28*8,4,1,0,bias=False),\n",
    "            nn.BatchNorm2d(28*8),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(28*8, 28*4,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(28*4),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(28*4, 28*2,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(28*2),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(28*2, 28,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(28),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.ConvTranspose2d(28,1,4,2,1),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            nn.Conv2d(1,1,3,3,10,bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.main(x)\n",
    "        \n",
    "\n",
    "#Discriminator Net\n",
    "class D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(1,1,3,3,10,bias=False),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "\n",
    "            \n",
    "            nn.Conv2d(1,28,4,2,1,bias=False),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            \n",
    "            nn.Conv2d(28,28*2,4,2,1,bias=False),\n",
    "            nn.BatchNorm2d(28*2),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "\n",
    "            nn.Conv2d(28*2, 28*4, 4, 2, 1,bias=False),\n",
    "            nn.BatchNorm2d(28*4),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            \n",
    "            nn.Conv2d(28*4, 28*8, 4, 2, 1,bias=False),\n",
    "            nn.BatchNorm2d(28*8),\n",
    "            nn.LeakyReLU(0.2,inplace=True),\n",
    "            \n",
    "            nn.Conv2d(28*8, 1, 4,1,0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.main(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0f83c5c3-17bd-4515-b3ba-fa9b8cf91bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save models\n",
    "torch.save(g,'GParams.pt')\n",
    "torch.save(d,'DParams.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f637660f-0556-4179-936e-85c82ddf7f29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "D(\n",
       "  (main): Sequential(\n",
       "    (0): ConvTranspose2d(1, 1, kernel_size=(3, 3), stride=(3, 3), padding=(10, 10), bias=False)\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Conv2d(1, 28, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (4): Conv2d(28, 56, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(56, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (7): Conv2d(56, 112, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (8): BatchNorm2d(112, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (9): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (10): Conv2d(112, 224, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (11): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (13): Conv2d(224, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "    (14): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Load models\n",
    "g = torch.load('GParams.pt')\n",
    "d = torch.load('DParams.pt')\n",
    "g.eval()\n",
    "d.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3dc1859b-1bee-4efd-a6e2-124706d1f234",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/469] LossD:1.4654333591461182, LossG:2.233675718307495, D(x):0.5366092324256897\n",
      "[20/469] LossD:0.5197416543960571, LossG:7.018393039703369, D(x):0.9337521195411682\n",
      "[40/469] LossD:0.17707961797714233, LossG:4.955238342285156, D(x):0.9198020696640015\n",
      "[60/469] LossD:0.14002734422683716, LossG:4.681378364562988, D(x):0.9140275716781616\n",
      "[80/469] LossD:0.6008049249649048, LossG:3.1430280208587646, D(x):0.6583072543144226\n",
      "[100/469] LossD:0.240045964717865, LossG:3.0082459449768066, D(x):0.9108560085296631\n",
      "[120/469] LossD:0.3023279309272766, LossG:4.6727800369262695, D(x):0.9482157826423645\n",
      "[140/469] LossD:0.17951054871082306, LossG:3.448505163192749, D(x):0.9424290060997009\n",
      "[160/469] LossD:0.2406102418899536, LossG:3.154723882675171, D(x):0.8868220448493958\n",
      "[180/469] LossD:0.3143535852432251, LossG:3.0316755771636963, D(x):0.8851892948150635\n",
      "[200/469] LossD:0.5751052498817444, LossG:2.3664724826812744, D(x):0.8514818549156189\n",
      "[220/469] LossD:0.4015578031539917, LossG:2.884913444519043, D(x):0.8408249020576477\n",
      "[240/469] LossD:1.0876078605651855, LossG:4.422431468963623, D(x):0.9925494194030762\n",
      "[260/469] LossD:0.6714680790901184, LossG:1.4932153224945068, D(x):0.6625481247901917\n",
      "[280/469] LossD:0.6194021701812744, LossG:1.271669864654541, D(x):0.6623393893241882\n",
      "[300/469] LossD:0.6237866282463074, LossG:1.3209487199783325, D(x):0.6674584150314331\n",
      "[320/469] LossD:0.3950992226600647, LossG:2.1245908737182617, D(x):0.8944212794303894\n",
      "[340/469] LossD:0.4664810299873352, LossG:1.8324823379516602, D(x):0.7776140570640564\n",
      "[360/469] LossD:0.4823002517223358, LossG:1.8952927589416504, D(x):0.7744060754776001\n",
      "[380/469] LossD:0.4077724814414978, LossG:2.5285654067993164, D(x):0.8458637595176697\n",
      "[400/469] LossD:0.5714296102523804, LossG:1.2171776294708252, D(x):0.639197587966919\n",
      "[420/469] LossD:0.46278443932533264, LossG:2.299391031265259, D(x):0.7100077867507935\n",
      "[440/469] LossD:0.45555388927459717, LossG:2.213132381439209, D(x):0.8247957825660706\n",
      "[460/469] LossD:0.7520222067832947, LossG:4.018539905548096, D(x):0.9282961487770081\n",
      "[0/469] LossD:1.5858559608459473, LossG:4.369792461395264, D(x):0.9850385189056396\n",
      "[20/469] LossD:0.6425108313560486, LossG:1.5691336393356323, D(x):0.6265007853507996\n",
      "[40/469] LossD:0.754745364189148, LossG:1.0551002025604248, D(x):0.5893518328666687\n",
      "[60/469] LossD:0.718305766582489, LossG:1.165208339691162, D(x):0.6272362470626831\n",
      "[80/469] LossD:1.349189043045044, LossG:0.8881686925888062, D(x):0.3626035749912262\n",
      "[100/469] LossD:0.9306074976921082, LossG:0.4666230082511902, D(x):0.5159848928451538\n",
      "[120/469] LossD:1.0149993896484375, LossG:2.845872163772583, D(x):0.8792548179626465\n",
      "[140/469] LossD:0.6800647974014282, LossG:1.7692546844482422, D(x):0.7782780528068542\n",
      "[160/469] LossD:1.2462396621704102, LossG:0.625475287437439, D(x):0.37090516090393066\n",
      "[180/469] LossD:0.8026401996612549, LossG:1.2495808601379395, D(x):0.5898674726486206\n",
      "[200/469] LossD:1.5810991525650024, LossG:3.3273940086364746, D(x):0.9653748273849487\n",
      "[220/469] LossD:0.6652461290359497, LossG:1.3761459589004517, D(x):0.7208715677261353\n",
      "[240/469] LossD:0.9779573678970337, LossG:2.440666913986206, D(x):0.8609932065010071\n",
      "[260/469] LossD:0.7147062420845032, LossG:1.2401057481765747, D(x):0.6693499684333801\n",
      "[280/469] LossD:0.6670109033584595, LossG:1.6906334161758423, D(x):0.7181600332260132\n",
      "[300/469] LossD:0.761245608329773, LossG:1.6594327688217163, D(x):0.7231330275535583\n",
      "[320/469] LossD:0.7428205013275146, LossG:1.0524123907089233, D(x):0.6257383823394775\n",
      "[340/469] LossD:0.7840567827224731, LossG:2.4567832946777344, D(x):0.8103764057159424\n",
      "[360/469] LossD:0.7104373574256897, LossG:1.980156660079956, D(x):0.8224168419837952\n",
      "[380/469] LossD:0.7696539163589478, LossG:0.8546157479286194, D(x):0.5785623788833618\n",
      "[400/469] LossD:0.6044779419898987, LossG:1.8344266414642334, D(x):0.7949804067611694\n",
      "[420/469] LossD:0.7061998844146729, LossG:1.2317136526107788, D(x):0.7060548067092896\n",
      "[440/469] LossD:0.7013616561889648, LossG:1.8162693977355957, D(x):0.8386341333389282\n",
      "[460/469] LossD:0.6905122399330139, LossG:1.349284052848816, D(x):0.6485949754714966\n",
      "[0/469] LossD:0.6744776964187622, LossG:1.1959272623062134, D(x):0.6587980389595032\n",
      "[20/469] LossD:0.736337423324585, LossG:1.1491670608520508, D(x):0.5996648669242859\n",
      "[40/469] LossD:0.6140645742416382, LossG:1.4804834127426147, D(x):0.7023509740829468\n",
      "[60/469] LossD:0.9467753171920776, LossG:3.1928093433380127, D(x):0.8929375410079956\n",
      "[80/469] LossD:0.6485368013381958, LossG:1.8094109296798706, D(x):0.7575179934501648\n",
      "[100/469] LossD:0.6969007849693298, LossG:1.4935585260391235, D(x):0.6213083267211914\n",
      "[120/469] LossD:0.5920209884643555, LossG:1.6846349239349365, D(x):0.7409511804580688\n",
      "[140/469] LossD:0.6733925938606262, LossG:1.2882792949676514, D(x):0.7317704558372498\n",
      "[160/469] LossD:0.8363549113273621, LossG:1.268560767173767, D(x):0.542906641960144\n",
      "[180/469] LossD:0.6151455640792847, LossG:1.7162485122680664, D(x):0.7207263708114624\n",
      "[200/469] LossD:0.6324613690376282, LossG:1.667543649673462, D(x):0.7186312079429626\n",
      "[220/469] LossD:0.7068324089050293, LossG:1.569319725036621, D(x):0.6681855916976929\n",
      "[240/469] LossD:0.5449556112289429, LossG:1.7543869018554688, D(x):0.7577805519104004\n",
      "[260/469] LossD:0.9429294466972351, LossG:1.0854637622833252, D(x):0.4800751805305481\n",
      "[280/469] LossD:0.6142259836196899, LossG:1.689051866531372, D(x):0.71172034740448\n",
      "[300/469] LossD:1.3646236658096313, LossG:3.6388187408447266, D(x):0.9397306442260742\n",
      "[320/469] LossD:0.648827075958252, LossG:1.5293457508087158, D(x):0.7126744985580444\n",
      "[340/469] LossD:0.6016154885292053, LossG:2.830308437347412, D(x):0.8886692523956299\n",
      "[360/469] LossD:0.599220335483551, LossG:1.5305309295654297, D(x):0.7355058193206787\n",
      "[380/469] LossD:0.6460974216461182, LossG:2.1178510189056396, D(x):0.849191427230835\n",
      "[400/469] LossD:1.6813639402389526, LossG:3.5552451610565186, D(x):0.9437301158905029\n",
      "[420/469] LossD:0.5206331610679626, LossG:2.0661184787750244, D(x):0.8199028968811035\n",
      "[440/469] LossD:0.6288256645202637, LossG:1.4545437097549438, D(x):0.6222751140594482\n",
      "[460/469] LossD:0.5367034673690796, LossG:1.9138157367706299, D(x):0.7310872077941895\n",
      "[0/469] LossD:0.5233527421951294, LossG:2.3456332683563232, D(x):0.8246190547943115\n",
      "[20/469] LossD:0.6736494898796082, LossG:1.5193167924880981, D(x):0.6569291353225708\n",
      "[40/469] LossD:0.5779988765716553, LossG:2.2179129123687744, D(x):0.8740047216415405\n",
      "[60/469] LossD:0.6231049299240112, LossG:1.2940080165863037, D(x):0.7081999778747559\n",
      "[80/469] LossD:0.6824198961257935, LossG:1.2670326232910156, D(x):0.6163578629493713\n",
      "[100/469] LossD:0.544320285320282, LossG:2.174556016921997, D(x):0.8688225150108337\n",
      "[120/469] LossD:0.5357016324996948, LossG:1.9031453132629395, D(x):0.7716271281242371\n",
      "[140/469] LossD:1.0754001140594482, LossG:0.8618553876876831, D(x):0.43347403407096863\n",
      "[160/469] LossD:0.5769350528717041, LossG:2.0708789825439453, D(x):0.7926106452941895\n",
      "[180/469] LossD:0.5086411237716675, LossG:2.082888603210449, D(x):0.7853655219078064\n",
      "[200/469] LossD:0.8379314541816711, LossG:4.027481555938721, D(x):0.9499607086181641\n",
      "[220/469] LossD:0.5832376480102539, LossG:1.775830864906311, D(x):0.7896934151649475\n",
      "[240/469] LossD:0.5863426327705383, LossG:2.2796876430511475, D(x):0.836091160774231\n",
      "[260/469] LossD:0.5781451463699341, LossG:2.398092031478882, D(x):0.8522571921348572\n",
      "[280/469] LossD:0.6002745628356934, LossG:1.8683576583862305, D(x):0.7428120970726013\n",
      "[300/469] LossD:0.623482346534729, LossG:2.1375598907470703, D(x):0.7564483880996704\n",
      "[320/469] LossD:0.5240312814712524, LossG:2.1048452854156494, D(x):0.8648961782455444\n",
      "[340/469] LossD:0.5643938183784485, LossG:1.3724677562713623, D(x):0.6765363216400146\n",
      "[360/469] LossD:0.43569114804267883, LossG:2.1997087001800537, D(x):0.7652896642684937\n",
      "[380/469] LossD:0.8124555349349976, LossG:2.776371479034424, D(x):0.9151343703269958\n",
      "[400/469] LossD:0.558976948261261, LossG:1.7376559972763062, D(x):0.683508038520813\n",
      "[420/469] LossD:1.0087864398956299, LossG:0.8750709295272827, D(x):0.4430725574493408\n",
      "[440/469] LossD:0.6449470520019531, LossG:2.968461513519287, D(x):0.8687391877174377\n",
      "[460/469] LossD:0.8753625154495239, LossG:1.1869933605194092, D(x):0.5004388689994812\n",
      "[0/469] LossD:0.8951212763786316, LossG:1.3218072652816772, D(x):0.5287438035011292\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [181]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m D_x \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     23\u001b[0m noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m fake \u001b[38;5;241m=\u001b[39m \u001b[43mg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m label\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     28\u001b[0m output \u001b[38;5;241m=\u001b[39m d(fake\u001b[38;5;241m.\u001b[39mdetach())\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [141]\u001b[0m, in \u001b[0;36mG.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:925\u001b[0m, in \u001b[0;36mConvTranspose2d.forward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# One cannot replace List by Tuple or Sequence in \"_output_padding\" because\u001b[39;00m\n\u001b[1;32m    921\u001b[0m \u001b[38;5;66;03m# TorchScript does not support `Sequence[T]` or `Tuple[T, ...]`.\u001b[39;00m\n\u001b[1;32m    922\u001b[0m output_padding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_padding(\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28minput\u001b[39m, output_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_transpose2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_padding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "g = G()\n",
    "d = D()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimG = torch.optim.Adam(g.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "optimD = torch.optim.Adam(d.parameters(),lr=0.0002,betas=(0.5,0.999))\n",
    "\n",
    "for epoch in range(5):\n",
    "    g_loss = 0\n",
    "    \n",
    "    for i,data in enumerate(dataloader,0):\n",
    "        \n",
    "        d.zero_grad()\n",
    "        b_size = data[0].size(0)\n",
    "        \n",
    "        label = torch.full((b_size,), 1, dtype=torch.float)\n",
    "        \n",
    "        output = d(data[0]).view(-1)\n",
    "        lossR = criterion(output,label)\n",
    "        lossR.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "        noise = torch.randn(128, 100, 1, 1)\n",
    "        \n",
    "        fake = g(noise)\n",
    "        label.fill_(0)\n",
    "        \n",
    "        output = d(fake.detach()).view(-1)\n",
    "        lossF = criterion(output,label)\n",
    "        lossF.backward()\n",
    "        \n",
    "        D_y = output.mean().item()\n",
    "        \n",
    "        loss = lossR + lossF\n",
    "        optimD.step()\n",
    "        \n",
    "        g.zero_grad()\n",
    "        label.fill_(1)\n",
    "        \n",
    "        output = d(fake).view(-1)\n",
    "        \n",
    "        lossG = criterion(output,label)\n",
    "        lossG.backward()\n",
    "        \n",
    "        D_z = output.mean().item()\n",
    "        \n",
    "        optimG.step()\n",
    "        \n",
    "        if i % 20 == 0:\n",
    "            print(f'[{i}/{len(dataloader)}] LossD:{loss}, LossG:{lossG}, D(x):{D_x}')\n",
    "            \n",
    "        if i > 460:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d93a7c89-e1eb-4a58-96fc-240fcaf836a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90912c79-1328-4370-8ce7-cc4baa151b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = T.ToPILImage()\n",
    "\n",
    "noise = torch.randn(1, 100, 1, 1)\n",
    "\n",
    "imgT = g(noise)\n",
    "\n",
    "img = transform(imgT[0])\n",
    "img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
